---
layout: page
title: About
permalink: /
---

News
----
Please help me promote this [channel called Friends in Education](https://www.youtube.com/channel/UCM5WZUS-lGaajtWo2AvSHmA). They are providing free online lectures for school going students who cannot go out during the Coronavirus lockdown.


About
-----
I have joined Google at it's Bangalore office. I am working in areas related to Machine Learning and Natural Language Processing. 

Before that, I was a Visiting Research Scholar at the [Machine Learning and Perception lab at Georgia Tech](http://mlp.cc.gatech.edu/) led by [Prof. Dhruv Batra](https://www.cc.gatech.edu/~dbatra/). I work on Machine Learning problems at the intersection of vision and language. Prior to that I was on a research internship at the [Statistics and Machine Learning Group at Indian Institute of Science, Bangalore](http://sml.csa.iisc.ernet.in/SML/). Before that I was working as a Platform Engineer at [Soroco](http://soroco.com/). I completed my Bachelor's in Computer Science and Engineering from the [Indian Institute of Technology Kharagpur](http://www.iitkgp.ac.in/) in May 2016.

<br>




## Select Awards / Honors
---------------

- Aug 2009 -- **NTSE Scholarship (National Talent Search Examination)** by Department of Science and Technology, Government of India
    - Awarded to less than 1% applicants based on their scholastic achievements.
- Aug 2011 -- **KVPY Fellow (Kishore Vaigyanik Protsahan Yojana Fellow)** by Department of Science and Technology, Government of India
    - Awarded to just ~100 students all over India with high academic potential.
    
- Sept. 2012 -- **Aditya Birla Scholarship** by Aditya Birla Group
    - Awarded to just 15 students all over the from the best engineering and management colleges of India.
    
- Feb 2013 -- **OP Jindal Engineering and Management Scholarship** by OP Jindal Group
    - Awarded to just 4 students from each of the few top-tier engineering and management colleges in India.

<br>



## Publications
--------------

<br>

<div class="container">
    

        <div class="row">
            <div class="col-md-12 col-sm-12 col-lg-12">
            <div class="row">
            
                <div class="col-md-12 col-sm-12 col-lg-12">
                    <b>Visual Landmark Selection for Generating Grounded and
                Interpretable Navigation Instructions.</b>
                </div>
                
            </div>
                
            <div class="row">
                
                <div class="col-md-12 col-sm-12 col-lg-12">
                
                   <b> S. Agarwal </b>, 
                    <a href="https://www.cc.gatech.edu/~parikh/" target="_blank">D. Parikh</a>,
                    <a href="https://www.cc.gatech.edu/~dbatra/" target="_blank">D. Batra</a>,
                    <a href="https://panderson.me/" target="_blank">P. Anderson</a>,
                    <a href="https://www.cc.gatech.edu/~slee3191/" target="_blank">S. Lee</a>
                    
                    <br>
    
                    CVPR 2019 Workshop on Deep Learning for Semantic Visual Navigation
                    <br>
                    [<a href="/files/cvpr-workshop.pdf" target="_blank">pdf</a>]
                    /
                    <a >code(to be released soon)</a>          
                </div>
            </div>
            </div>
            
    </div>

    <div class="row">
        <div class="col-md-4 col-sm-4 col-lg-4">
            <div><img align="left" src="/images/projects/cvpr_model.png" width="100%"></div>
            <div style="text-align: center;">Two-stage instruction generation</div>
        </div>
        <div class="col-md-8 col-sm-8 col-lg-8">
            <div style="font-size:14px">
                    <br>
                    <br>
                    Instruction following for vision-and-language navigation
                    (VLN) has prompted significant research efforts developing
                    more powerful “follower” models since its inception.
                    However, the inverse task of generating visually grounded
                    instructions given a trajectory – or learning a “speaker”
                    model – has been largely unexamined. We present a “speaker”
                    model that generates navigation instructions in two stages,
                    by first selecting a series of discrete visual landmarks
                    along a trajectory using hard attention, and then second
                    generating language instructions conditioned on these
                    landmarks. This two-stage approach improves over prior work,
                    while also permitting greater interpretability. We hope to
                    extend this to a reinforcement learning setting where
                    landmark selection is optimized to maximize a follower’s
                    performance without disrupting the model’s language fluency.
                
            </div>
        </div>
    </div>
    
    




<br>
<br>




    
    
                
        <div class="row">
            <div class="col-md-12 col-sm-12 col-lg-12">
            <div class="row">
            
                <div class="col-md-12 col-sm-12 col-lg-12">
                    <b>AppTechMiner: Mining Applications and Techniques from Scientific Articles</b>
                </div>
                
            </div>
                
            <div class="row">
                
                <div class="col-md-12 col-sm-12 col-lg-12">
                
                    M. Singh,
                    S. Dan,
                    <b> S. Agarwal </b>, 
                    P. Goyal,
                    A. Mukherjee,
                    
                    <br>
    
                    Joint Conference on Digital Libraries (JCDL) 2017:6th International Workshop On Mining Scientific Publications
                    <br>
                    [<a href="https://arxiv.org/abs/1709.03064" target="_blank">pdf</a>]                   
                </div>
            </div>
            </div>
            
    </div>

    <div class="row">
        <div class="col-md-4 col-sm-4 col-lg-4">
            <div><img align="left" src="/images/projects/apptechminer.png" width="100%"></div>
            <div style="text-align: center;"> A sample Phrase-Cloud representing the proportion of papers for an area.</div>
        </div>
        <div class="col-md-8 col-sm-8 col-lg-8">
            <br>
            <br>
            <div style="font-size:14px">
                A rule-based information extraction framework that automatically constructs a knowledge base of all application areas and problem solving "techniques", given a text corpus of research papers. "Techniques" include tools, methods, datasets or evaluation metrics. We also categorize individual research articles based on their application areas and the techniques proposed/improved in the article. Our system achieves high average precision (~82%) and recall (~84%) in knowledge base creation. It also performs well in application and technique assignment to an individual article (average accuracy ~66%). We demonstrated the framework for the domain of computational linguistics but this can be easily generalized to any other field of research.
                
            </div>
        </div>
    </div>
    
    
    
</div>


<br>
<br>


## Patents
-----------

#### SYSTEMS AND METHODS FOR EXECUTING SOFTWARE ROBOT COMPUTER PROGRAMS ON VIRTUAL MACHINES

Publication number: 20180189093

Abstract: Techniques for executing one or more instances of a computer program using virtual machines, the computer program comprising multiple computer program portions including a first computer program portion. The techniques include determining whether an instance of any of the multiple computer program portions is to be executed; when it is determined that a first instance of the first computer program portion is to be executed, accessing first information specifying a first set of one or more virtual machine resources required for executing the first instance of the first computer program portion; determining whether any one of the plurality of virtual machines has at least the first set of virtual machine resources available; and when it is determined that a first of the plurality of virtual machines has the first set virtual machine resources available, causing the first virtual machine to execute the first instance of the first computer program portion.

Filed: January 5, 2018

Publication date: July 5, 2018

Inventors: **Sanyam Agarwal**, Rohan Narayan Murty, George Peter Nychis, Wolfgang Richter, Nishant Kumar Jain, Surabhi Mour, Shreyas H. Karanth, Shashank Anand


<br>


## Select Projects
-------------

<div class="container">

    

    
                
    <div class="row">
        <div class="col-md-12 col-sm-12 col-lg-12">
        <div class="row">
        
            <div class="col-md-12 col-sm-12 col-lg-12">
                <b>Fast GPU-Based Simulator for Room-to-Room dataset</b>
            </div>
            
        </div>
            
        <div class="row">
            
            <div class="col-md-12 col-sm-12 col-lg-12">
                <a >code(to be released)</a>
            </div>
        </div>
        </div>
            
    </div>
        
        
    

    <div class="row">
        <div class="col-md-4 col-sm-4 col-lg-4">
            <div><img align="left" src="/images/projects/r2r.gif" width="100%"></div>
            <div style="text-align: center;"> Sample trajectory in Room-to-Room dataset</div>
        </div>
        <div class="col-md-8 col-sm-8 col-lg-8">
            <div style="font-size:14px">
                <br>
                <br>
                Room-to-Room <a href="https://bringmeaspoon.org/" target="_blank"> dataset </a> is a commond dataset used in several vision and language navigation tasks. The dataset contains real-world panoramic scans of building interiors provided my the <a href="https://niessner.github.io/Matterport/" target="_blank"> Matterport3D dataset </a>. An agent can choose to move in this 3D environment by taking actions. I optimized the original <a href="https://github.com/peteanderson80/Matterport3DSimulator" target="_blank"> Room-to-Room simulator </a>
                to use GPU for state update operations when the agent takes any actions. Combined with caching of repeated computations this resulted in a simulator that is <b> 17x </b> faster frame rate on a single GPU! Training time of most vision-and-language models can be brought down significantly using this simulator instead of the original simulator.
                
            </div>
        </div>
    </div>


<br>
<br>


    <div class="row">
        <div class="col-md-12 col-sm-12 col-lg-12">
        <div class="row">
        
            <div class="col-md-12 col-sm-12 col-lg-12">
                <b>ICLR 2018 reproducibility challenge: Interpretable Counting for Visual Question Answering</b>
            </div>
            
        </div>
            
        <div class="row">
            
            <div class="col-md-12 col-sm-12 col-lg-12">
                [<a href="https://github.com/sanyam5/irlc-vqa-counting" target="_blank">code</a>]
                
            </div>
        </div>
        </div>
            
    </div>
        


    <div class="row">
        <div class="col-md-4 col-sm-4 col-lg-4">
            <div><img align="left" src="/images/projects/irlc.gif" width="100%"></div>
            <div style="text-align: center;"> Ques: How many sheepskin are grazing?</div>
        </div>
        <div class="col-md-8 col-sm-8 col-lg-8">
            <div style="font-size:14px">
                <br>
                <br>
                The paper improves upon the state-of-the art accuracy for counting based questions in VQA. They do it by enforcing the prior that each count corresponds to a well defined region in the image and is not diffused all over it. They hard-attend over a fixed set of candiate regions (taken from pre-trained Faster-R-CNN network) in the image by fusing it with the information from the question. They use a variant of REINFORCE - Self Critical Training - which is well suited for generating sequences.
                
            </div>
        </div>
    </div>


<br>
<br>


    <div class="row">
        <div class="col-md-12 col-sm-12 col-lg-12">
        <div class="row">
        
            <div class="col-md-12 col-sm-12 col-lg-12">
                <b>The first public PyTorch implementations of Skip-Thought Vectors</b>
            </div>
            
        </div>
            
        <div class="row">
            
            <div class="col-md-12 col-sm-12 col-lg-12">
                [<a href="https://github.com/sanyam5/skip-thoughts" target="_blank">code</a>]
                [<a href="http://sanyam5.github.io/my-thoughts-on-skip-thoughts/" target="_blank">blog</a>]
            </div>
        </div>
        </div>
            
    </div>
        
        
    

    <div class="row">
        <div class="col-md-4 col-sm-4 col-lg-4">
            <div><img align="left" src="/images/projects/skip-thoughts.png" width="100%"></div>
            <div style="text-align: center;"> Skip-Thoughts model</div>
        </div>
        <div class="col-md-8 col-sm-8 col-lg-8">
            <div style="font-size:14px">
                <br>
                <br>
                Skip-Thoughts uses the order of the sentences to “self-supervise” itself. The underlying assumption here is that whatever, in the content of a sentence, leads to a better reconstruction of the neighbouring sentences is also the essence of the sentence.

The decoders are trained to minimise the reconstruction error of the previous and the next sentences given the embedding of current sentence. This reconstruction error is back-propagated to the Encoder which must now pack as much information about the current sentence that will help the Decoders minimise the error in generating the previous and next sentences.
                
            </div>
        </div>
    </div>



<br>
<br>


    <div class="row">
        <div class="col-md-12 col-sm-12 col-lg-12">
        <div class="row">
        
            <div class="col-md-12 col-sm-12 col-lg-12">
                <b>The first public PyTorch implementations of Attentive Recurrent Comparators</b>
            </div>
            
        </div>
            
        <div class="row">
            
            <div class="col-md-12 col-sm-12 col-lg-12">
                [<a href="https://github.com/sanyam5/arc-pytorch" target="_blank">code</a>]
                [<a href="http://sanyam5.github.io/understanding-attentive-recurrent-comparators/" target="_blank">blog</a>]
            </div>
        </div>
        </div>
            
    </div>
        
        
    

    <div class="row">
        <div class="col-md-4 col-sm-4 col-lg-4">
            <div><img align="left" src="/images/projects/arc1.gif" width="100%"></div>
            <div style="text-align: center;"> ARC comparing between two images of the same character</div>
        </div>
        <div class="col-md-8 col-sm-8 col-lg-8">
            <div style="font-size:14px">
                <br>
                <br>
                Attentive Recurrent comparators state of the art performance in one-shot learning on the Omniglot dataset. The key idea is to make the training phase as close as possible to evaluation phase. Since the evaluation phase is comparitive, making the training phase also comparitive will lead to better generalization. In training, the model learns to compare between two randomly chosen images of either same or different characters.
                
            </div>
        </div>
    </div>




<br>
<br>


    <div class="row">
        <div class="col-md-12 col-sm-12 col-lg-12">
        <div class="row">
        
            <div class="col-md-12 col-sm-12 col-lg-12">
                <b>Robot Soccer, IIT Kharagpur</b>
            </div>
            
        </div>
            
        <div class="row">
            
            <div class="col-md-12 col-sm-12 col-lg-12">
                [<a href="https://krssg.in/index.html" target="_blank" >Group Website</a>]
            </div>
        </div>
        </div>
            
    </div>
        
        
    

    <div class="row">
        <div class="col-md-4 col-sm-4 col-lg-4">
            <div><img align="left" src="/images/projects/fira2k15.jpg" width="100%"></div>
            <div style="text-align: center;"> Bronze medal at FIRA 2015, South Korea</div>
        </div>
        <div class="col-md-8 col-sm-8 col-lg-8">
            <div style="font-size:14px">
            
            <br>
                Led the Artificial Intelligence team. Created a framework for learning robot control policies for playing soccer using Q-Learning. Our group won Bronze medal in FIRA 2015, South Korea.
                
            </div>
        </div>
    </div>

    
</div>


